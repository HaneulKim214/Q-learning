{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import math\n",
    "import numpy as np\n",
    "\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "from sklearn.preprocessing import KBinsDiscretizer\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_learning(history, policy_name):\n",
    "    fig = go.Figure()\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x = [i for i in range(len(history))],\n",
    "            y = history,\n",
    "        )\n",
    "    )\n",
    "\n",
    "    fig.update_yaxes(title_text=\"Final rewards\")\n",
    "    fig.update_xaxes(title_text=\"i_games\")\n",
    "    fig.update_layout(title_text=policy_name.upper())\n",
    "    \n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Goal of Reinforcement learning is to find an optimal policy that will lead to most reward.\n",
    "\n",
    "In this tutorial we will go from random sampling to Q-learning which is a model-free learning. By going step by step we will clearly understand limitations of each methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Policy 1: Random Sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our example of CartPole-v1, there are only two actions at any given state therefore we will pick \"left\" or \"right\" at random."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_policy():\n",
    "    action = np.random.choice(env.action_space.n)\n",
    "    return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "num_games = 100\n",
    "history = []\n",
    "\n",
    "state = env.reset()\n",
    "env.render()\n",
    "\n",
    "for game_i in range(num_games):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    final_reward = 0\n",
    "    while not done:\n",
    "        action = random_policy()\n",
    "        next_s, reward, done, info = env.step(action)\n",
    "        final_reward += reward\n",
    "        env.render()\n",
    "        \n",
    "    history.append(final_reward)\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy_name = 'random policy'\n",
    "fig = plot_learning(history, policy_name)\n",
    "fig.write_html(f\"CartPole-v1_{policy_name}.html\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotting how the agent performs on 100 games we can see that its policy is outputting random rewards. Next we will try to develop our policy to obtain greater reward."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Policy 2: Follow tilt\n",
    "\n",
    "This policy makes cart move in the direction of the pole "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def follow_tilt(state):\n",
    "    \"\"\"Given current state information move cart in direction of the pole tilt\"\"\"\n",
    "    pole_angle = state[2]\n",
    "    if pole_angle > 0:\n",
    "        action = 1\n",
    "    else:\n",
    "        action = 0\n",
    "    return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_games = 100\n",
    "history = []\n",
    "state = env.reset()\n",
    "env.render()\n",
    "\n",
    "for game_i in range(num_games):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    final_reward = 0\n",
    "    while not done:\n",
    "        action = follow_tilt(state)\n",
    "        next_s, reward, done, info = env.step(action)\n",
    "        final_reward += reward\n",
    "        state = next_s\n",
    "        env.render()\n",
    "        \n",
    "    history.append(final_reward)\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy_name = 'follow_tilt policy'\n",
    "fig = plot_learning(history, policy_name)\n",
    "fig.write_html(f\"CartPole-v1_{policy_name}.html\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even though it does better than our random policy we've created a policy using our knowledge of the environment. In most cases knowledge of the environment is not given and also this policy is only suitable for this particular type of environment, not generalized well enough.\n",
    "\n",
    "To make our policy generalizable we will use __Q-learning__ which is a __model free__(No knowledge of environment is needed) policy to maximize our pole and see if it indeed generalizes to other problems. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Policy 3: Q-learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "def q_choose_action(state, epsilon):\n",
    "    \"\"\"Given state, use q_learning policy to take action\"\"\"\n",
    "    if np.random.uniform() < epsilon:\n",
    "        action = np.random.choice(2)\n",
    "    else:\n",
    "        q_vals = q_table[state]\n",
    "        action = np.argmax(q_vals)\n",
    "#         perm_actions = np.random.permutation(2)\n",
    "#         q_vals = [q_vals[a] for a in perm_actions] # randomizing q_vals -> why?\n",
    "#         perm_q_argmax = np.argmax(q_vals)\n",
    "#         action = perm_actions[perm_q_argmax]\n",
    "    return action\n",
    "\n",
    "def update_q(transition):\n",
    "    \"\"\"Given information and after taking action following q_learning policy we update our q_table\"\"\"\n",
    "    s, a, r, next_s, done = transition\n",
    "    q_val = q_table[s][a] #여기서 꺼내온걸 q_target을 사용하여 업데이트(learn) 해준다\n",
    "    if done:\n",
    "        q_target = r\n",
    "    else:\n",
    "        q_target = r + discount_value*np.max(q_table[next_s])\n",
    "\n",
    "    q_table[s][a] += learning_rate*(q_target - q_val)\n",
    "    return q_table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "q_table => for each possible state we should have probabilities of all possible actions, i.e, for each unique angle and pole_velocity pair(state) it should have its own associated action probabilities which continuously get updated using q_learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_bins = ( 6 , 12 )\n",
    "lower_bounds = [ env.observation_space.low[2], -math.radians(50) ]\n",
    "upper_bounds = [ env.observation_space.high[2], math.radians(50) ]\n",
    "\n",
    "est = KBinsDiscretizer(n_bins=n_bins, encode='ordinal', strategy='uniform')\n",
    "est.fit([lower_bounds, upper_bounds ])\n",
    "\n",
    "def discretizer(cp, cv, angle, pole_velocity):\n",
    "    \"\"\"Convert continues state intro a discrete state\"\"\"\n",
    "    return tuple(map(int, est.transform([[angle, pole_velocity]])[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "n_action_space = 2\n",
    "learning_rate = 0.01 #alpha\n",
    "discount_value = 0.9 #gamma\n",
    "epsilon = 0.9\n",
    "epsilon_min = 0.1\n",
    "epsilon_decay = 0.95\n",
    "q_table = np.zeros(n_bins + (n_action_space, ))\n",
    "\n",
    "num_games = 100\n",
    "history = []\n",
    "q_tables = []\n",
    "for game_i in range(num_games):\n",
    "    if game_i % 10 == 0:\n",
    "        q_tables.append(q_table)\n",
    "    state = discretizer(*env.reset()) # passing observations of resetted state\n",
    "    done = False\n",
    "    final_reward = 0\n",
    "    env.render()\n",
    "    while not done:\n",
    "        action = q_choose_action(state, epsilon)\n",
    "        next_s, reward, done, info = env.step(action)\n",
    "        next_s = discretizer(*next_s)\n",
    "        q_table = update_q((state, action, reward, next_s, done), q_table)# q_table doesn't seem to get updated....\n",
    "        \n",
    "        if epsilon > epsilon_min:\n",
    "            epsilon *= epsilon_decay\n",
    "        \n",
    "        final_reward += reward\n",
    "        state = next_s\n",
    "        env.render()\n",
    "        \n",
    "    history.append(final_reward)\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ True,  True],\n",
       "        [ True,  True],\n",
       "        [ True,  True],\n",
       "        [ True,  True],\n",
       "        [ True,  True],\n",
       "        [ True,  True],\n",
       "        [ True,  True],\n",
       "        [ True,  True],\n",
       "        [ True,  True],\n",
       "        [ True,  True],\n",
       "        [ True,  True],\n",
       "        [ True,  True]],\n",
       "\n",
       "       [[ True,  True],\n",
       "        [ True,  True],\n",
       "        [ True,  True],\n",
       "        [ True,  True],\n",
       "        [ True,  True],\n",
       "        [ True,  True],\n",
       "        [ True,  True],\n",
       "        [ True,  True],\n",
       "        [ True,  True],\n",
       "        [ True,  True],\n",
       "        [ True,  True],\n",
       "        [ True,  True]],\n",
       "\n",
       "       [[ True,  True],\n",
       "        [ True,  True],\n",
       "        [ True,  True],\n",
       "        [ True,  True],\n",
       "        [ True,  True],\n",
       "        [ True,  True],\n",
       "        [ True,  True],\n",
       "        [ True,  True],\n",
       "        [ True,  True],\n",
       "        [ True,  True],\n",
       "        [ True,  True],\n",
       "        [ True,  True]],\n",
       "\n",
       "       [[ True,  True],\n",
       "        [ True,  True],\n",
       "        [ True,  True],\n",
       "        [ True,  True],\n",
       "        [ True,  True],\n",
       "        [ True,  True],\n",
       "        [ True,  True],\n",
       "        [ True,  True],\n",
       "        [ True,  True],\n",
       "        [ True,  True],\n",
       "        [ True,  True],\n",
       "        [ True,  True]],\n",
       "\n",
       "       [[ True,  True],\n",
       "        [ True,  True],\n",
       "        [ True,  True],\n",
       "        [ True,  True],\n",
       "        [ True,  True],\n",
       "        [ True,  True],\n",
       "        [ True,  True],\n",
       "        [ True,  True],\n",
       "        [ True,  True],\n",
       "        [ True,  True],\n",
       "        [ True,  True],\n",
       "        [ True,  True]],\n",
       "\n",
       "       [[ True,  True],\n",
       "        [ True,  True],\n",
       "        [ True,  True],\n",
       "        [ True,  True],\n",
       "        [ True,  True],\n",
       "        [ True,  True],\n",
       "        [ True,  True],\n",
       "        [ True,  True],\n",
       "        [ True,  True],\n",
       "        [ True,  True],\n",
       "        [ True,  True],\n",
       "        [ True,  True]]])"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q_tables[0] == q_tables[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy_name = 'Q-learning'\n",
    "fig = plot_learning(history, policy_name)\n",
    "fig.write_html(f\"CartPole-v1_{policy_name}.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "References:\n",
    "- https://www.youtube.com/watch?v=JNKvJEzuNsc&t=64s"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
